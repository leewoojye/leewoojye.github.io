---
layout: post
title:  "Direct Preference Optimization (DPO)"
date:   2026-01-27 20:52:23 +0900
comments: true
categories: research
---

5장에서 말하는 '동등성(Equivalence)'

예시: 두 명의 면접관에 대해

면접관 A: "철수는 90점, 영희는 80점이야. 철수가 더 잘했네."

면접관 B: "철수는 50점, 영희는 40점이야. 철수가 더 잘했네."

두 면접관의 점수 절대값은 다르지만, "누가 더 우수한가?"라는 결론은 같다. 5장의 보상 동등성 증명은 "점수를 어떻게 매기든, 우열 관계만 똑같다면 AI는 결국 똑같은 행동을 배우게 된다"는 것을 보여주며, 그래서 DPO는 복잡한 절대 점수 대신, 두 답변 사이의 상대적인 확률 차이만 신경 쓴다.

$$\hat{r}_{\theta}(x,y) = \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$$
