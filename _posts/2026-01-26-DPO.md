---
layout: post
title:  "Direct Preference Optimization (DPO)"
date:   2026-01-27 20:52:23 +0900
comments: true
categories: research
---

5장에서 말하는 '동등성(Equivalence)'

예시: 두 명의 면접관에 대해

면접관 A: "철수는 90점, 영희는 80점이야. 철수가 더 잘했네."

면접관 B: "철수는 50점, 영희는 40점이야. 철수가 더 잘했네."

두 면접관의 점수 절대값은 다르지만, "누가 더 우수한가?"라는 결론은 같다. 5장의 보상 동등성 증명은 "점수를 어떻게 매기든, 우열 관계만 똑같다면 AI는 결국 똑같은 행동을 배우게 된다"는 것을 보여주며, 그래서 DPO는 복잡한 절대 점수 대신, 두 답변 사이의 상대적인 확률 차이만 신경 쓴다.

$$\hat{r}_{\theta}(x,y) = \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$$

### DPO for image segmentation
$$L_{DPO} = -\log \sigma (\beta [(s_{\theta}^+ - s_{\theta}^-) - (s_{ref}^+ - s_{ref}^-)])$$

### 기존 DPO 손실함수를 segmentation task에 맞게 변형하기
$$L_{DPO} = -\log \sigma (\beta [(s_{\theta}^+ - s_{\theta}^-) - (s_{ref}^+ - s_{ref}^-)]) + \alpha [-(s_{\theta}^+ + s_{ref}^+) + (s_{ref}^- + s_{\theta}^-)]$$

4개의 항을 모델이 균형 있게 고려하게 하는 법?
1. Log-Sum-Exp 또는 소프트맥스 정규화
- 수식 구조: $L_{anchor} = \log \left( e^{-(s_{\theta}^+ + s_{ref}^+)} + e^{(s_{ref}^- + s_{\theta}^-)} \right)$
2. Hinge Margin 적용
Threshold를 설정하는 방법으로, 각 항이 목표한 점수에 도달하면 더 이상 손실을 주지 않음으로써, 이미 잘 학습된 항보다는 부족한 항에 모델이 집중하도록 강제함
- 양성 항: $\max(0, \tau - s_\theta^+)$
- 음성 항: $\max(0, s_\theta^- + \tau)$

$$L_{anchor} = \max(0, \tau - s_{\theta}^+) + \max(0, s_{\theta}^- + \tau)$$

### reference
- Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment (NeurIPS 2024)
  - 양성($s^+$)과 음성($s^-$)의 차이는 벌어지는데 정작 양성 샘플의 절대적인 Likelihood는 같이 떨어지는 현상에서 착안
  - Calibration(교정)을 위한 Hinge Loss항을 추가하면서 $\tau$ (Threshold) 을 자연스럽게 사용

$$L_{Cal-DPO} = L_{DPO} + \alpha \cdot L_{cal}$$

$$L_{cal} = \mathbb{E}_{(x, y_w) \sim D} \left[ \max(0, \tau - \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}) \right]$$

- Rethinking DPO: The Role of Rejected Responses in Preference Misalignment (ACL 2025)
  - DPO가 선호 답변($y_w$)의 확률을 높이기보다 비선호 답변($y_l$)의 확률을 깎는 데 훨씬 더 많은 에너지를 쓴다는 사실에서 착안 -> 정답($y_w$)의 확률을 직접적으로 높여주는 NLL(Negative Log-Likelihood) 항을 추가

$$L_{total} = L_{DPO} + \alpha L_{NLL}(y_w)$$

$$L_{NLL}(y_w) = -\log \pi_{\theta}(y_w|x)$$

- Understanding Learning from Human Preferences (DeepMind, 2024)
  - 차이가 일정 수준 이상 벌어지면 손실을 주지 않는 상수 마진을 도입하여, 모델이 불필요하게 '차이'에만 집착하지 않도록 조절 -> 오버피팅 방지