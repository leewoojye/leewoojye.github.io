---
layout: post
title:  "Direct Preference Optimization (DPO)"
date:   2026-01-27 20:52:23 +0900
comments: true
categories: research
---

### PPO
PPO는 이름에 들어간 Proximal(근접한)이라는 단어처럼, 새로운 정책이 학습 이전의 기존 정책에서 너무 멀리 벗어나지 않도록 제한하면서 업데이트를 진행한다.

논문의 Preliminaries에서 설명하듯, 기존의 RLHF 파이프라인은 3단계로 구성되며 PPO는 마지막 단계에서 핵심 역할을 한다.
- SFT(Supervised Fine-Tuning): 고품질 데이터로 모델을 먼저 학습시켜 $\pi_{SFT}$를 얻습니다.
- Reward Modeling: 인간의 선호도 데이터를 기반으로 보상 모델 $$\r_\phi(x, y)$$를 학습시킵니다.
- RL Fine-Tuning(PPO 단계): 학습된 보상 모델의 점수를 최대화하도록 언어 모델\(\pi_\theta\)을 미세 조정합니다.이때 PPO가 최적화하려는 목적 함수(Objective Function)는 다음과 같습니다.

$$\max_{\pi_\theta} E_{x \sim D, y \sim \pi_\theta (y|x)} [r_\phi(x, y)] - \beta D_{KL} [\pi_\theta(y|x) || \pi_{ref}(y|x)]$$

- \(r_\phi(x, y)\): 보상 모델이 주는 점수입니다. 이를 최대화하여 인간이 좋아하는 답변을 하도록 만듦
- \(D_{KL} [\pi_\theta || \pi_{ref}]\): 현재 학습 중인 모델(\(\pi_\theta\))이 기존 모델(\(\pi_{ref}\))에서 너무 멀어지지 않게 만드는 제약 조건(KL-Divergence)
- \(\beta\): 보상과 제약 사이의 균형을 조절하는 하이퍼파라미터

$$D_{KL} (\pi_\theta(y|x) \, \Vert \, \pi_{ref}(y|x))$$

5장에서 말하는 '동등성(Equivalence)'

예시: 두 명의 면접관에 대해

면접관 A: "철수는 90점, 영희는 80점이야. 철수가 더 잘했네."

면접관 B: "철수는 50점, 영희는 40점이야. 철수가 더 잘했네."

두 면접관의 점수 절대값은 다르지만, "누가 더 우수한가?"라는 결론은 같다. 5장의 보상 동등성 증명은 "점수를 어떻게 매기든, 우열 관계만 똑같다면 AI는 결국 똑같은 행동을 배우게 된다"는 것을 보여주며, 그래서 DPO는 복잡한 절대 점수 대신, 두 답변 사이의 상대적인 확률 차이만 신경 쓴다.

$$\hat{r}_{\theta}(x,y) = \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$$

### DPO for image segmentation
$$L_{DPO} = -\log \sigma (\beta [(s_{\theta}^+ - s_{\theta}^-) - (s_{ref}^+ - s_{ref}^-)])$$

### 기존 DPO 손실함수를 segmentation task에 맞게 변형하기
$$L_{DPO} = -\log \sigma (\beta [(s_{\theta}^+ - s_{\theta}^-) - (s_{ref}^+ - s_{ref}^-)]) + \alpha [-(s_{\theta}^+ + s_{ref}^+) + (s_{ref}^- + s_{\theta}^-)]$$

4개의 항을 모델이 균형 있게 고려하게 하는 법?
1. Log-Sum-Exp 또는 소프트맥스 정규화
- 수식 구조: $L_{anchor} = \log \left( e^{-(s_{\theta}^+ + s_{ref}^+)} + e^{(s_{ref}^- + s_{\theta}^-)} \right)$
2. Hinge Margin 적용
Threshold를 설정하는 방법으로, 각 항이 목표한 점수에 도달하면 더 이상 손실을 주지 않음으로써, 이미 잘 학습된 항보다는 부족한 항에 모델이 집중하도록 강제함
- 양성 항: $\max(0, \tau - s_\theta^+)$
- 음성 항: $\max(0, s_\theta^- + \tau)$

$$L_{anchor} = \max(0, \tau - s_{\theta}^+) + \max(0, s_{\theta}^- + \tau)$$

### reference
- Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment (NeurIPS 2024)
  - 양성($s^+$)과 음성($s^-$)의 차이는 벌어지는데 정작 양성 샘플의 절대적인 Likelihood는 같이 떨어지는 현상에서 착안
  - Calibration(교정)을 위한 Hinge Loss항을 추가하면서 $\tau$ (Threshold) 을 자연스럽게 사용

$$L_{Cal-DPO} = L_{DPO} + \alpha \cdot L_{cal}$$

$$L_{cal} = \mathbb{E}_{(x, y_w) \sim D} \left[ \max(0, \tau - \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}) \right]$$

- Rethinking DPO: The Role of Rejected Responses in Preference Misalignment (ACL 2025)
  - DPO가 선호 답변($y_w$)의 확률을 높이기보다 비선호 답변($y_l$)의 확률을 깎는 데 훨씬 더 많은 에너지를 쓴다는 사실에서 착안 -> 정답($y_w$)의 확률을 직접적으로 높여주는 NLL(Negative Log-Likelihood) 항을 추가

$$L_{total} = L_{DPO} + \alpha L_{NLL}(y_w)$$

$$L_{NLL}(y_w) = -\log \pi_{\theta}(y_w|x)$$

- Understanding Learning from Human Preferences (DeepMind, 2024)
  - 차이가 일정 수준 이상 벌어지면 손실을 주지 않는 상수 마진을 도입하여, 모델이 불필요하게 '차이'에만 집착하지 않도록 조절 -> 오버피팅 방지