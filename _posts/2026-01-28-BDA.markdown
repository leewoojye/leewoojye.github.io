---
layout: post
title:  "Bi-Directional Attention"
date:   2026-01-28 18:35:21 +0900
comments: true
categories: research
---

### BDA: Bi-directional attention for zero-shot learning
위 논문을 참고하여 작성하였습니다.

본 논문에서는 

BDA의 이점

비전 특징 ↔ 시맨틱 속성(텍스트/attribute) 사이를 양방향으로 어텐션시켜서, 두 공간을 서로 조건부로 보게 만든다. 그 결과, seen 클래스에서 학습된 visual–semantic 관계를 더 정교하게 정렬하게 만들어 unseen으로 넘어갈 때 domain shift를 줄어들게 된다.

![alt text](/assets/images/image-13.png)
![alt text](/assets/images/image-14.png)

<!-- VSA(Visual Synthesis Attention)가 의미론적 정보(속성 정의)를 구체적으로 다듬는 과정은 **'속성별 맞춤형 시각 증거 추출'**과 **'피드포워드 네트워크(FFN)를 통한 정교화'**라는 두 가지 핵심 메커니즘을 통해 이루어집니다.논문에 제시된 수식과 구조를 바탕으로 한 단계별 과정은 다음과 같습니다.
1. 속성 중심의 질의(Query) 설정VSA는 일반적인 시각 처리를 넘어, 모델이 알고 있는 **속성 정보($\mathcal{V}_A$)**를 직접적인 **질의(Query, $Q^V$)**로 사용합니다.모델은 "내가 알고 있는 '노란색 부리'라는 개념($Q^V$)에 해당하는 실제 데이터가 이 이미지의 어디에 있는가?"라고 묻습니다.
2. 2. 시각적 증거와의 대조 및 가중치 계산이 질의($Q^V$)를 SSA 단계에서 정제된 **시각 특징($U^S$)**인 **키(Key, $K^V$)**와 대조합니다.이 과정에서 소프트맥스(Softmax) 함수를 통해 이미지의 각 격자(Grid) 영역이 해당 속성과 얼마나 관련이 있는지 계산하여 '어텐션 맵'을 생성합니다.결과적으로 이미지 내에서 속성을 가장 잘 설명하는 구체적인 시각적 위치와 특징을 찾아내게 됩니다.
3. 3. 국소적 시각 특징 합성 (Locality-Synthesis)찾아낸 위치 정보를 바탕으로 실제 시각적 **값(Value, $V^V$)**들을 합산하여 새로운 특징 벡터($Z^V$)를 만듭니다.이를 통해 추상적인 텍스트 정의였던 속성 정보가 **"이 이미지의 특정 영역에서 추출된 구체적인 시각적 증거"**가 결합된 형태로 변합니다.논문에서는 이를 **'국소적 합성 시각 특징(Locality-synthesized visual features)'**이라고 정의합니다.
4. FFN을 통한 비선형 정교화합성된 특징은 마지막으로 두 개의 선형 층과 ReLU 활성화 함수로 구성된 **피드포워드 네트워크(FFN)**를 통과합니다.이 단계는 시각적 디테일이 반영된 속성 정보를 한 번 더 변별력 있게 가공하여, 비슷한 속성들(예: '노란색 부리' vs '주황색 부리') 사이의 미세한 차이를 더 잘 구분할 수 있도록 지식을 정교화합니다.
5. 결과물: $F$ (Refined Features)이 과정을 거쳐 탄생한 최종 특징 **$F$**는 단순한 이미지 데이터도, 단순한 텍스트 데이터도 아닙니다. 이는 **"이미지의 구체적인 시각적 맥락이 주입되어 정교하게 다듬어진 속성 지식"**이며, 이를 통해 모델은 처음 보는 클래스(Unseen class)에 대해서도 매우 높은 예측 정확도를 달성하게 됩니다. -->
  
<!-- 
1. $U^S$(Semantic-Augmented Visual Features)는 SSA 모듈의 최종 출력값으로, "이미지 특징($U$)에 의미 정보($\mathcal{V}_A$)를 주입하여 강화한 시각 특징"입니다.
수식적 정의: $U^S \leftarrow U + Z^S$. 여기서 $Z^S$는 이미지 특징을 Query로, 속성 임베딩을 Key/Value로 사용하여 계산된 합성 특징입니다.\
역할: 속성 데이터($\mathcal{V}_A$)의 도움을 받아 이미지 내에서 각 속성과 가장 관련이 깊은 영역을 찾아내고, 해당 부분의 시각적 특징을 강조한 상태입니다. 즉, **"의미론적으로 중요한 부위가 어디인지"**가 반영된 시각 데이터입니다.
2. $U^V$ (Locality-Synthesized Visual Features)$U^V$는 VSA 모듈의 출력값으로, "정제된 시각 정보($U^S$)를 바탕으로 다시 한번 다듬어진 합성 특징"입니다.
수식적 정의: $U^V = \mathcal{V}_A + Z^V$. 여기서 $Z^V$는 속성 임베딩을 Query로, 앞서 만든 $U^S$를 Key/Value로 사용하여 계산됩니다.
역할: SSA에서 찾은 구체적인 시각적 디테일을 사용하여 추상적인 속성 정보를 정교화합니다. 이 과정을 통해 모델은 단순히 '텍스트'로만 알던 속성을 실제 '이미지 상의 형태'와 결합하게 됩니다.
활용: $U^V$는 이후 **FFN(Feed-Forward Network)**을 통과하여 최종 특징 $F$가 되며 , 모델의 최적화를 돕는 **시각 기반 교차 엔트로피 손실($\mathcal{L}_{VCE}$)**을 계산하는 데 직접 사용됩니다. 

SSA, VSA를 거쳐 어텐션 맵이 누적되어 합쳐(합성)지는데 비전조건부-시멘틱 관계, 시멘틱조건부-비전 관계 두 상관성을 regularization으로 건 모습과 유사하다고 느꼈다.
-->
구조적 의존성: VSA(Visual Synthesis Attention)는 SSA(Semantic Synthesis Attention)의 결과물을 재료로 사용 (SSA가 선행되어야 성능이 높아짐)
[Recall] SSA -> VSA 흐름
### SSA
- 시각적 특징을 Query로 하여 속성(Value)와의 어텐션을 거칩니다.
- 원본 시각적 특징에 어텐션 계산 결과를 더해 속성에 해당하는 부위를 부각하는 정제된 시각 이미지를 만듭니다.
- $U^S = U + Z^S$
### VSA
- 이 정제된 시각 이미지를 Value로 하여, 원본 속성(Query)과 어텐션을 거칩니다.
- 어텐션 계산 결과를 현재 속성 $\mathcal{V}_A$에 더해주며 이는 속성 정보를 업데이트하는 효과를 냅니다.
- $U^V = \mathcal{V}_A + Z^V$

중간 결과물을 계속 누적하는 것으로 보아 어텐션 결과를 '합성'한다는 표현이 잘 어울리고 마치 정규화항과 여러 손실함수를 더해 하나의 손실함수로 만드는 과정같기도 합니다.

다소 신기했던 부분은(그러기에 처음에 무진장 헷갈렸던) 서로 다른 모달리티에 대해 어텐션 결과를 구분하지 않고 '합성'했다는 점입니다. visual을 query로 하여 얻은 어텐션 맵과 semantic을 query로 하여 얻은 어텐션 맵을 이유는 모르겠지만 '당연히 둘은 분리해야지!' 생각으로 읽었다가 시간만 지체했네요 .. 어쨋든 

또 눈여겨 볼 부분은 VSA보다 선행되는 SSA입니다. 실험적으로 SSA가 선행될 때 유의미한 성능 향상을 보인다고 합니다. 

가장 먼저 떠오르는 원인으로 이미지 chunk와 속성 chunk 간 불균형한 규모가 떠올랐습니다. 이미지가 쿼리인 경우 쿼리가 바라봐야 할 key의 가짓수가 적으니(예시로 등장한 속성은 약 3-4 단어 정도) saliency map을 만들기가 비교적 수월하지 않았을까 합니다. 반대로 속성이 쿼리였다면 속성 중심 어텐션 결과 안에 모든 이미지 chunk의 특징이 응집되게 되므로, 집중했어야 할 이미지의 시선도 분산되기 쉬워 보이네요!
<!-- VSA 출력으로 속성 형상을 하면 좋은 점 : 이미지 형상($HW \times C$)으로 남겨두면 정보가 흩어져 있지만, 속성 형상($A \times C$)으로 변환하면 이 이미지에서 '부리'라는 속성을 가장 잘 설명하는 시각적 디테일"이 하나의 벡터로 응축됩니다 
띠용~
-->

![alt text](/assets/images/image-15.png)
그리고 이미지(차원: HWxC) 쿼리와 속성(AxC) 키를 어텐션했을 때 왜 결과가 HWxC지? 머리가 막막했는데요 네 그냥 어텐션 매커니즘 까먹은 1인이었습니다 .. (복습하자!)

속성을 쿼리로 어텐션할 경우 한 속성 쿼리에 대해 모든 이미지 청크 키와의 유사도를 구하게 됩니다. 이때 이미지 청크 갯수만큼의 attention score가 나오게 되고 softmax 등의 함수를 거쳐 어텐션 스코어에 대한 확률분포(attention distribution)를 만듭니다. 그리고 어텐션 스코어의 기댓값이 그 쿼리에 대한 attention output이 되는데, attention distribution를 이용해 attention score를 가중합하면 attention score의 기댓값, 즉 우리가 구하고 싶은 어텐션 결과를 얻을 수 있습니다.

각 속성 쿼리마다 하나의 어텐션 결과를 얻으므로 최종 어텐션 결과는 AxC 형상을 갖게 됩니다!

![alt text](/assets/images/image-16.png)

또 어텐션이 수행되는 SSA, VSA 블록은 Q,K,V로의 변환을 위한 학습가능한 가중치 행렬이 존재합니다. 역전파로 VSA의 $W^Q$를 업데이트하는 건 모델의 속성이해능력을 향상시키는 셈입니다.
<!-- 최종 출력 $U^V = (U + Z^S) + Z^V$은 강화된 이미지 표현($Z^S$), 강화된 속성 표현($Z^V$)이 포함되어 있으므로 $U^V$를 바탕으로 역전파를 수행하면  -->
참고로 제로샷 학습(ZSL)의 최종 목적은 이미지 정보를 의미 공간(Semantic Space)으로 옮겨와 분류하는 것입니다.