---
layout: post
title: "Parameter-Efficient Fine-Tuning (PEFT)"
date: 2026-02-15 21:53:01 +0900
comments: true
categories: research
---

2. 주요 PEFT 방법론PEFT는 접근 방식에 따라 크게 세 가지 카테고리로 나뉩니다.① Re-parameterization (재매개변수화) - LoRA현재 가장 널리 쓰이는 방식인 **LoRA (Low-Rank Adaptation)**가 대표적입니다. 가중치의 업데이트량($\Delta W$)을 직접 학습하는 대신, 두 개의 작은 저차원 행렬($A, B$)로 분해하여 학습합니다.원리: 고정된 원래 가중치 $W$에 대해 $W + \Delta W = W + BA$로 정의하며, 여기서 $A$와 $B$만 학습합니다.수식: $W \in \mathbb{R}^{d \times k}, B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}$ (이때 $r \ll d, k$)② Additive (추가형) - Adapters & Prompt Tuning기존 모델의 구조는 그대로 두고, 새로운 레이어나 벡터를 추가하여 해당 부분만 학습합니다.Adapters: Transformer 블록 내부에 작은 Bottleneck 레이어(Adapter)를 삽입합니다.Prompt/Prefix Tuning: 입력 데이터 앞에 학습 가능한 '가상 토큰(Soft Prompt)'을 붙여 모델의 출력을 제어합니다.