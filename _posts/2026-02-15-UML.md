---
layout: post
title: "Metric Learning과 UML"
date: 2026-02-15 15:53:51 +0900
comments: true
categories: research
---

UML에서 단일 모델이 학습하고자 하는 것들
1. 공통적인 시각 언어 (Shared Visual Language)
새와 자동차는 전혀 다르지만, 이를 구성하는 '기본 요소'는 공유합니다.

무엇을 읽나: "노란색 새"와 "노란색 자동차"가 있을 때, 모델은 이들이 '노란색'이라는 색상 특징을 공유함을 이해합니다. 
의미: 모델은 특정 데이터셋에만 국한된 특징이 아니라, 세상 모든 사물을 설명할 수 있는 **보편적인 시각 특징(색상, 질감, 선의 흐름 등)**을 학습하게 됩니다.

2. 범주 간의 거리감 (Semantic Hierarchy)
단일 모델로 학습하면 모든 사물이 하나의 거대한 '지도' 위에 배치됩니다.

무엇을 읽나: 모델은 명시적으로 배우지 않아도 **"새와 개(생물)는 자동차나 비행기(기계)보다 서로 더 가깝다"**는 거대한 관계를 파악합니다.
의미: 개별 모델을 쓸 때는 알 수 없었던 데이터셋 간의 상대적 위치가 정해집니다. 이 지도 위에서 "생물 영역", "탈것 영역", "의류 영역" 같은 거대한 클러스터(뭉치)가 형성되는 것을 읽을 수 있습니다.

3. 도메인별 중요도 차이 (Attention Shift)
이것이 UML의 핵심인데, 모델이 데이터에 따라 무엇에 집중할지를 결정하는 관계를 읽습니다.

무엇을 읽나: 
새를 볼 때는 "색깔이 중요해(종 구분에 핵심이니까)"라고 판단합니다.
자동차를 볼 때는 "색깔보다는 외형의 곡선이 중요해(같은 모델이라도 색은 다를 수 있으니까)"라고 판단합니다.


의미: 모델이 **"이 데이터셋(도메인)에서는 이런 특징이 핵심이다"**라는 데이터셋별 맞춤형 대응 능력을 갖게 됩니다. 논문에서 언급된 'Feature Variability' 문제를 해결하면서 얻게 되는 통찰입니다.

---
Stochastic Adapter를 Transformer 블록마다 병렬(Parallel)로 설계한 이유는 크게 세 가지 관점에서 설명할 수 있습니다. 이는 단순히 효율성을 넘어, 이 논문이 해결하고자 하는 **통합 메트릭 학습(UML)**의 핵심 과제인 '데이터 불균형'과 '편향'을 해결하기 위한 전략적 선택입니다.
1. 사전 학습된 지식의 파괴 방지 (Preservation of Pre-trained Knowledge)

직렬(Sequential) 구조와의 차이: 일반적인 어댑터가 Transformer 레이어 뒤에 직렬로 붙으면, 사전 학습된 모델의 출력이 무조건 어댑터를 거쳐야 하므로 원래의 특징(Feature)이 왜곡될 가능성이 큽니다.
병렬의 장점: 병렬 구조는 고정된(Frozen) 사전 학습 모델의 경로를 그대로 유지합니다. 모델은 **"사전 학습된 일반적인 특징"**에 **"어댑터가 학습한 특정 데이터셋의 특징"**을 잔차(Residual) 방식으로 더하게 됩니다. 이 방식은 📄 AdaptFormer에서 입증되었듯, 원래 모델이 가진 일반화 능력을 해치지 않으면서 새로운 지식을 유연하게 결합할 수 있게 합니다.

수정(Modification)"이 아닌 "보정(Residual Adding) :

직렬 구조: 신호가 Frozen Layer -&gt; Adapter -&gt; Next Layer 순으로 흐르므로, 어댑터가 입력값 전체를 받아서 변형시킨 후 다음 레이어로 넘깁니다.
병렬 구조: Frozen Layer의 결과물에 어댑터가 계산한 **작은 차이값(Delta, \DeltaΔ\DeltaΔ)**을 더하는 방식입니다. 
UML처럼 데이터 분포가 극단적으로 다를 때는, 원래의 특징을 유지하면서 살짝 '보정'만 해주는 병렬 방식이 전체적인 학습 안정성(Stability)을 유지하는 데 훨씬 유리합니다.

Frozen 레이어에 주는 영향의 최소화 :

직렬 구조: 앞쪽 레이어의 어댑터가 신호를 변형시키면, 뒤쪽에 있는 고정된(Frozen) 레이어들은 자신이 원래 배우지 않았던 생소한 분포의 입력을 받게 됩니다.
병렬 구조: 각 블록의 어댑터가 해당 블록의 출력에만 관여하므로, 뒤쪽 레이어들이 받는 입력값의 분포 변화가 상대적으로 완만합니다. 이는 대규모 사전 학습 모델의 지식을 최대한 활용해야 하는 Parameter-efficient 학습에서 매우 중요한 요소입니다.

1. 확률적 제어(Stochasticity)의 최적화

On/Off 제어의 용이성: 병렬 구조에서는 어댑터를 끄는 행위(\gamma = 0γ=0\gamma = 0γ=0)가 단순히 해당 경로의 출력을 더하지 않는 것이 됩니다. 이 경우 해당 레이어는 순수하게 사전 학습된 상태로 돌아갑니다.
균형 잡힌 학습: 훈련 중에 확률적으로 어댑터를 끄면, 모델은 특정 데이터셋(특히 샘플이 많은 대규모 데이터셋)에만 맞춰진 어댑터의 값에 전적으로 의존할 수 없게 됩니다. 이는 어댑터가 "없어도" 메트릭을 잘 계산해야 한다는 제약을 주어, 모델이 특정 도메인에 편향되는 것을 강하게 억제합니다.